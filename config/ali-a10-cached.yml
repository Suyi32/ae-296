world_size: 1
loras_num: 8
token_per_lora: 8
input_len: 64 # max input length supported, constrained by CPU numbers
swapper_ids: [0, 1]
model_dir: '/root/.cache/huggingface/hub/models--huggyllama--llama-7b/snapshots/8416d3fefb0cb3ff5775a7b13c1692d10ff1aa16'
mode: 'aaas-cached'
hidden_dim: 4096
